{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HumEnv Tutorial\n",
    "\n",
    "This tutorial covers basic usage of HumEnv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All imports\n",
    "\n",
    "It is important to set the relevant MuJoCo rendering environment variables before importing humenv. If rendering is not needed, you can skip setting the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MUJOCO_GL\"] = os.environ.get(\"MUJOCO_GL\", \"egl\")\n",
    "os.environ[\"PYOPENGL_PLATFORM\"] = os.environ[\"MUJOCO_GL\"]\n",
    "\n",
    "from pathlib import Path\n",
    "import mediapy as media\n",
    "import sys\n",
    "import inspect\n",
    "import numpy as np\n",
    "import json\n",
    "from gymnasium.wrappers import FlattenObservation, TransformObservation\n",
    "\n",
    "# humenv\n",
    "import humenv\n",
    "from humenv import make_humenv\n",
    "from humenv.env import make_from_name\n",
    "from humenv import rewards as humenv_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HumEnv basics\n",
    "\n",
    "`make_humenv` is a custom environment creation function. It returns a tuple `(env, manager)`. This has a similar interface to the standard `gymnasium.make_vec` interface. \n",
    "\n",
    "\n",
    "The manager is used to share objects between processes (see this [link](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.Manager)). For example, we use it to share the dataset of motions between different environments when using multiprocessing. Manager can be `None` (when not used) or `multiprocessing.Manager()`. If manager is not `None` you should call `shutdown` before exiting the application.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a single environment\n",
    "env, manager = make_humenv()\n",
    "print(f\"environment: {env}\")\n",
    "print(f\"manager: {manager}\")\n",
    "\n",
    "# As in gymnasium, `close` should be called when the environment is not needed anymore\n",
    "env.close()\n",
    "if manager is not None:\n",
    "    manager.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by default the environment is wrapped in a `TimeLimit` wrapper since we have a fix horizon of 300 steps. This can be easily changed by passing `max_episode_steps` to the make_humenv function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, manager = make_humenv(max_episode_steps=1000)\n",
    "print(f\"time step: {env.spec.max_episode_steps}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create parallel environments and we use the standard `gymnasium` wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in [\"sync\", \"async\"]:\n",
    "    print(mode)\n",
    "    env, manager = make_humenv(num_envs=4, vectorization_mode=\"async\") # multiprocess\n",
    "    print(f\"environment: {env}\")\n",
    "    print(f\"manager: {manager}\")\n",
    "    env.close()\n",
    "    if manager is not None:\n",
    "        manager.shutdown()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Environment creation options.** \n",
    "\n",
    "Let's take a look on the options available during HumEnv creation:\n",
    "* `num_envs` - the number of requested environments\n",
    "* `motions` - list of motion file names used to sample initiali positions if MoCap is used\n",
    "* `motion_base_path` - where to look for motion files\n",
    "* `wrappers` - a sequence of Gymansium wrappers to be applied for created env(s)\n",
    "* `env_kwargs` - are passed to the HumEnv constructor, which in turns accepts:\n",
    "* `task` -  reward function object or string that can instantiate such object.\n",
    "* `xml` - the xml definition of the robot to be used, default to the tuned HumEnv robot\n",
    "* `state_init` - the way random seed is generated\n",
    "* `seed` - random seed\n",
    "* `fall_prob` - the probability of fall initialization in a mixed initalization mode\n",
    "\n",
    "    As well as rendering options passed to MuJoCo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation and Rollouts\n",
    "\n",
    "The observation in HumEnv is a dictionary containing the proprioceptive state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, _ = make_humenv()\n",
    "print(\"Observation space\")\n",
    "for k,v in env.observation_space.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(f\"Action space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now interact with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "frames = [env.render()]\n",
    "for i in range(60):\n",
    "    env.step(env.action_space.sample())\n",
    "    frames.append(env.render())\n",
    "media.show_video(frames, fps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Why did we set `fps=30`?*\n",
    "\n",
    "The control frequency of `HumEnv` is 30Hz. But the integration timestep is smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"dt={env.unwrapped.model.opt.timestep}\")\n",
    "print(f\"action_repeat={env.unwrapped.action_repeat}\")\n",
    "print(f\"control frequency: {env.unwrapped.model.opt.timestep * env.unwrapped.action_repeat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine our environment with existing gymnasium wrappers. A useful transformer is the recently introduced [`NumpyToTorch`](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.NumpyToTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, _ = make_humenv(\n",
    "    num_envs=1,\n",
    "    wrappers=[\n",
    "        FlattenObservation,\n",
    "        lambda env: TransformObservation(env, lambda obs: obs.reshape(1, -1), None),\n",
    "    ],\n",
    "    seed=1,\n",
    ")\n",
    "obs, info = env.reset()\n",
    "print(f\"observation: {type(obs)}, {obs.shape}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics information and reset\n",
    "\n",
    "The *physics* state of the Mujoco HumEnv simulator is given by\n",
    "* Position: qpos\n",
    "* Velocity: qvel\n",
    "\n",
    "The state is entirely encapsulated in the `mjData` struct that can be accessed from `env.data`. We expose this information through the `info` dictionary in `reset` and `step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, _ = make_humenv()\n",
    "_, info = env.reset()\n",
    "for k, v in info.items():\n",
    "    print(f\"{k}: {v.shape} -> {v[:11]}\")\n",
    "print((info[\"qpos\"] == env.unwrapped.data.qpos).all())\n",
    "print((info[\"qvel\"] == env.unwrapped.data.qvel).all())\n",
    "\n",
    "print('-'*10)\n",
    "_, _, _, _, info = env.step(env.action_space.sample())\n",
    "for k, v in info.items():\n",
    "    print(f\"{k}: {v.shape} -> {v[:11]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables are important for resetting HumEnv in a particular situation and computing the reward. So it is important to keep track of these variables. We will see about the reward later on. Let's start with resetting the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "frame_0 = env.render()\n",
    "new_qpos = np.array([0.13769039,-0.20029453,0.42305034,0.21707786,0.94573617,0.23868944\n",
    ",0.03856998,-1.05566834,-0.12680767,0.11718296,1.89464102,-0.01371153\n",
    ",-0.07981451,-0.70497424,-0.0478,-0.05700732,-0.05363342,-0.0657329\n",
    ",0.08163511,-1.06263979,0.09788937,-0.22008936,1.85898192,0.08773695\n",
    ",0.06200327,-0.3802791,0.07829525,0.06707749,0.14137152,0.08834448\n",
    ",-0.07649805,0.78328658,0.12580912,-0.01076061,-0.35937259,-0.13176489\n",
    ",0.07497022,-0.2331914,-0.11682692,0.04782308,-0.13571422,0.22827948\n",
    ",-0.23456622,-0.12406075,-0.04466465,0.2311667,-0.12232673,-0.25614032\n",
    ",-0.36237662,0.11197906,-0.08259534,-0.634934,-0.30822742,-0.93798716\n",
    ",0.08848668,0.4083417,-0.30910404,0.40950143,0.30815359,0.03266103\n",
    ",1.03959336,-0.19865537,0.25149713,0.3277561,0.16943092,0.69125975\n",
    ",0.21721349,-0.30871948,0.88890484,-0.08884043,0.38474549,0.30884107\n",
    ",-0.40933304,0.30889523,-0.29562966,-0.6271498])\n",
    "\n",
    "env.unwrapped.set_physics(qpos=new_qpos, qvel=np.random.rand(75), ctrl=np.zeros(69)) # qvel and ctrl are optionals\n",
    "# ctrl correspond to the action\n",
    "\n",
    "# we can see that we moved the environment in a new state\n",
    "media.show_image(np.concatenate([frame_0, env.render()], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "\n",
    "HumEnv comes with multiple parametric reward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = inspect.getmembers(sys.modules[\"humenv.rewards\"], inspect.isclass)\n",
    "for reward_class_name, reward_cls in all_rewards:\n",
    "    if not inspect.isabstract(reward_cls):\n",
    "        print(reward_class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads to a potentially infinite set of tasks. We provide a list of predefined reward function in `humenv.ALL_TASKS` which is a list of string. The reward class exposes a function to instantiate the tasks from their string-based name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of predefined tasks: {len(humenv.ALL_TASKS)}\")\n",
    "print(\"Examples:\")\n",
    "print(humenv.LOCOMOTION_TASKS[:10])\n",
    "print(humenv.ROTATION_TASKS[:10])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can instantiate a reward using their string-based representation or directly from using the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_fn = make_from_name(\"jump-2\")\n",
    "print(reward_fn)\n",
    "print(humenv_rewards.JumpReward(jump_height=1.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reward class exposes a function `compute(model: mujoco.MjModel, data: mujoco.MjData)` that computes the rewards from the physics state of the simulator. Since `data` can be reconstructed using `qpos`, `qvel` and `ctrl`, we expose a call function `__call__(model: mujoco.MjModel, qpos: np.ndarray, qvel: np.ndarray, ctrl: np.ndarray)` This is why it is important to keep track of these variables if you plan to reset the environment or recompute the reward.\n",
    " **Note that `ctrl` correspond to the action.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, _ = make_humenv(task=\"move-ego-0-0\")\n",
    "env.reset()\n",
    "action = env.action_space.sample()\n",
    "_, reward, _, _, _ = env.step(action)\n",
    "reward_fn = env.unwrapped.task\n",
    "print(reward, \", reward from step\")\n",
    "print(\"Reward recomputation:\")\n",
    "print(reward_fn.compute(env.unwrapped.model, env.unwrapped.data), \", reward from model and data [Ok]\")\n",
    "print(reward_fn(env.unwrapped.model, env.unwrapped.data.qpos, env.unwrapped.data.qvel, np.zeros(69)), \", we passed the wrong action, the reward is not correct [No]\")\n",
    "print(reward_fn(env.unwrapped.model, env.unwrapped.data.qpos, env.unwrapped.data.qvel, action), \", we passed the correct action, the reward is correct [Ok]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Motion Capture (MoCap) data\n",
    "\n",
    "HumEnv supports motions from the AMASS dataset. The datasets needs to be separately downloaded and preprocessed with the [following instructions](data_preparation/README.md). We assume in the examples below that the prepared data exists in its default location:\n",
    "```\n",
    "data_preparation/humenv_amass\n",
    "```\n",
    "\n",
    "## Using MoCap to for the initial state setting\n",
    "\n",
    "HumEnv supports the following initial states of the environment:\n",
    "\n",
    "* Default - T-pose\n",
    "* Fall - Random rotation of perturbed T-pose, may lead to unavoidable fall \n",
    "* MoCap - Random pose from the provided MoCap dataset\n",
    "* DefaultAndFall - Random mixture of Default and Fall\n",
    "* MoCapAndFall - Random mixture of MoCap and Fall\n",
    "\n",
    "Below we present a MoCapAndFall initalization using the default location of the pre-processed data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"state_init\":\"MoCap\",}\n",
    "motions_base_path = Path(\"data_preparation/humenv_amass\")\n",
    "if motions_base_path.exists():\n",
    "    #Let's use only 10 motions for faster loading:\n",
    "    motions = [str(x.name) for x in motions_base_path.glob(\"*.hdf5\")][0:10]\n",
    "    env, _ = make_humenv(motion_base_path=str(motions_base_path), motions=motions, **kwargs)  \n",
    "    frame = env.render()\n",
    "    media.show_image(frame)\n",
    "else:\n",
    "    print(\"[WARNING] You should generate the data before running these instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HumEnv Benchmark\n",
    "\n",
    "HumEnv provides three benchmarking protocols:\n",
    "\n",
    "* Reward (`humenv.bench.RewardEvaluation`) - evaluate on a set of reward tasks with pre-defined, paramterized objective functions\n",
    "* Goal (`humenv.bench.GoalEvaluation`) - evaluate on a a set of poses \n",
    "* Tracking (`humenv.bench.TrackingEvaluation`) - evaluate on the set of motions from the MoCap dataset\n",
    "\n",
    "The benchmark is optional and requires additional dependencies. It should be installed using `pip install humenv[bench]`.\n",
    "\n",
    "Let's define first a RandomAgent that we will evaluate in the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env=env\n",
    "    def act(self, obs, z):\n",
    "        return self.env.action_space.sample()\n",
    "    def reward_inference(self, task):\n",
    "        return None\n",
    "    def goal_inference(self, goal_pose):\n",
    "        return None\n",
    "    def tracking_inference(self, next_obs):\n",
    "        return None\n",
    "\n",
    "num_envs = 2\n",
    "env , _= make_humenv(num_envs=num_envs)\n",
    "random_agent = RandomAgent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Evaluation\n",
    "\n",
    "Reward evaluation allows to test an agent on a renge of reward tasks. It is highly customizable. In the following example:\n",
    "\n",
    "- `tasks=['move-ego-0-0', 'jump-2']` means stand and jump\n",
    "- `num_envs = 2` means we will create a vectorized environments with 2 environments\n",
    "- `vectorization_mode='sync'` means we will use `gymnasium.vector.SyncVectorEnv`\n",
    "- `num_episodes=4` means that in total, we will run 4 episodes of each task.\n",
    "- `state_init=Fall` means that the episode starts from a random initial falling position.\n",
    "\n",
    "The result of the evaluation is the `metrics` dictionary, which contains total reward and length for per episode for each task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from humenv.bench import RewardEvaluation\n",
    "\n",
    "reward_eval = RewardEvaluation(\n",
    "        tasks=['move-ego-0-0', 'jump-2'],\n",
    "        env_kwargs={ # this dictionary is passed to the environment constructor\n",
    "            \"state_init\": \"Fall\", # random falling initialization\n",
    "        }, \n",
    "        num_contexts=1, # the random agent is not a task conditioned agent so we use only one context\n",
    "        num_envs=num_envs,\n",
    "        num_episodes=4,\n",
    "        vectorization_mode='sync'\n",
    "    )\n",
    "metrics = reward_eval.run(agent=random_agent)\n",
    "print()\n",
    "print(\"Results\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"task='{k}'\")\n",
    "    for k2, v2 in v.items():\n",
    "        print(f\"  {k2}: {v2}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal Evaluation\n",
    "\n",
    "The overall structure is similar to reward benchmarking discussed above, so let's focus on differences. In goal evaluation, the agent should get as quickly as possible, and stay as close as possible to the goal pose.\n",
    "\n",
    "The goal poses are prepared during AMASS data process, and are available by default in the file `data_preparation/goal_poses/goals.json`.  `GoalEvaluation` works as `RewardEvaluation`, but instead of list of tasks, it accepts dictionary of goal poses, formatted as `{\"pose_name\": observation_array}`.\n",
    "\n",
    "Metrics for goal evaluation include reward (by default zero) and length, but also success (was the pose ever achieved with some tolerance), proximity (how muhc time humanoid spent within the tolerance threshold), and distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from humenv.bench import GoalEvaluation\n",
    "\n",
    "pose_file = \"data_preparation/goal_poses/goals.json\"\n",
    "with open(pose_file, \"r\") as json_file:\n",
    "    goals = json.load(json_file)\n",
    "for pose_name, payload in goals.items():\n",
    "    goals[pose_name] = np.array(payload[\"observation\"])\n",
    "\n",
    "# shortlist for fast execution\n",
    "shortlisted_goals = {k: goals[k] for k in [\"t_pose\", \"handstand\"]}\n",
    "\n",
    "goal_eval = GoalEvaluation(\n",
    "        goals=shortlisted_goals,\n",
    "        env_kwargs={\n",
    "            \"state_init\": \"Default\",\n",
    "        },\n",
    "        num_contexts=1,\n",
    "        num_envs=num_envs,\n",
    "        num_episodes=4,\n",
    "        vectorization_mode='async'\n",
    "    )\n",
    "metrics = goal_eval.run(agent=random_agent)\n",
    "print()\n",
    "print(\"Results\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"task='{k}'\")\n",
    "    for k2, v2 in v.items():\n",
    "        print(f\"  {k2}: {v2}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Evaluation\n",
    "\n",
    "The tracking benchmarking had more constrained protocol than the other ones: the humanoid is set to the first pose of a motion loaded from a MoCap file, and its objective is to stay as close to the next frame recorded in the MoCap trajectory. The length of the episodes is variable, and for a deterministic policy and deterministic simulator, such as MuJoCo, it does not make sense to repeat the episode for a given motion. \n",
    "\n",
    "In this example we take 5 first available motions, and evaluate them. `num_envs` regulates on how many workers the computation is spread.\n",
    "\n",
    "The metrics set includes proximity, distance, success, emd (earth's mover distance), and a set of additional metric used in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from humenv.bench import TrackingEvaluation\n",
    "\n",
    "num_envs = 1\n",
    "# env is needed here only for RandomAgent\n",
    "env, _ = make_humenv(num_envs=num_envs)\n",
    "random_agent = RandomAgent(env)\n",
    "\n",
    "\n",
    "motions_base_path = Path(\"data_preparation/humenv_amass\")\n",
    "if motions_base_path.exists():\n",
    "    #Let's use only 5 motions for faster loading:\n",
    "    motions = [str(x.name) for x in motions_base_path.glob(\"*.hdf5\")][0:5]\n",
    "\n",
    "    tracking_eval = TrackingEvaluation(\n",
    "            motions=motions,\n",
    "            num_envs=num_envs,\n",
    "            motion_base_path=motions_base_path,\n",
    "        )\n",
    "    metrics = tracking_eval.run(agent=random_agent)\n",
    "    print()\n",
    "    print(\"Results\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"task='{k}'\")\n",
    "        for k2, v2 in v.items():\n",
    "            print(f\"  {k2}: {v2}\")\n",
    "    env.close()\n",
    "else:\n",
    "    print(\"[WARNING] You should generate the data before running these instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
