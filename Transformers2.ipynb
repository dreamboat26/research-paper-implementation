{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZg_Bn23Z7lR",
        "outputId": "422f624c-019d-4380-df1e-1313c92e3ab7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traditional Transformer Output Shape: torch.Size([10, 32, 256])\n",
            "Transformer2 Output Shape: torch.Size([10, 32, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Traditional Transformer Block\n",
        "class TraditionalTransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TraditionalTransformerBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attn_output, _ = self.attention(query, key, value, attn_mask=mask)\n",
        "        x = self.dropout(self.norm1(attn_output + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "# Transformer2 Block with Singular Value Fine-Tuning (SVF)\n",
        "class Transformer2Block(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(Transformer2Block, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # Expert vector for singular value scaling\n",
        "        self.svf_expert = nn.Parameter(torch.ones(embed_size))\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attn_output, _ = self.attention(query, key, value, attn_mask=mask)\n",
        "        scaled_attn = attn_output * self.svf_expert.unsqueeze(0)  # Apply SVF scaling\n",
        "        x = self.dropout(self.norm1(scaled_attn + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "embed_size = 256\n",
        "heads = 8\n",
        "dropout = 0.1\n",
        "forward_expansion = 4\n",
        "\n",
        "# Dummy input\n",
        "dummy_input = torch.rand(10, 32, embed_size)  # (sequence_length, batch_size, embed_size)\n",
        "mask = None\n",
        "\n",
        "# Traditional Transformer Output\n",
        "traditional_block = TraditionalTransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "traditional_output = traditional_block(dummy_input, dummy_input, dummy_input, mask)\n",
        "\n",
        "# Transformer2 Output\n",
        "transformer2_block = Transformer2Block(embed_size, heads, dropout, forward_expansion)\n",
        "transformer2_output = transformer2_block(dummy_input, dummy_input, dummy_input, mask)\n",
        "\n",
        "print(\"Traditional Transformer Output Shape:\", traditional_output.shape)\n",
        "print(\"Transformer2 Output Shape:\", transformer2_output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Singular Value Fine-Tuning (SVF) Module\n",
        "class SVFExpert(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(SVFExpert, self).__init__()\n",
        "        self.scale_vector = nn.Parameter(torch.ones(embed_size))\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        return input_tensor * self.scale_vector.unsqueeze(0)\n",
        "\n",
        "# Transformer2 Block with SVF and Expert Vectors\n",
        "class Transformer2Block(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion, num_experts):\n",
        "        super(Transformer2Block, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_size, embed_size)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.experts = nn.ModuleList([SVFExpert(embed_size) for _ in range(num_experts)])\n",
        "\n",
        "    def select_expert(self, input_tensor):\n",
        "        # Simple heuristic: pick an expert based on the mean value of the input\n",
        "        expert_idx = int(input_tensor.mean().item() * len(self.experts)) % len(self.experts)\n",
        "        return self.experts[expert_idx]\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attn_output, _ = self.attention(query, key, value, attn_mask=mask)\n",
        "        expert = self.select_expert(attn_output)\n",
        "        scaled_attn = expert(attn_output)\n",
        "        x = self.dropout(self.norm1(scaled_attn + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "# Transformer2 Model\n",
        "class Transformer2Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, heads, dropout, forward_expansion, num_experts):\n",
        "        super(Transformer2Model, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.layers = nn.ModuleList([\n",
        "            Transformer2Block(embed_size, heads, dropout, forward_expansion, num_experts)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        out = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "        logits = self.fc_out(out)\n",
        "        return logits\n",
        "\n",
        "# Parameters\n",
        "vocab_size = 1000\n",
        "embed_size = 256\n",
        "num_layers = 4\n",
        "heads = 8\n",
        "dropout = 0.1\n",
        "forward_expansion = 4\n",
        "num_experts = 3\n",
        "\n",
        "# Initialize model\n",
        "model = Transformer2Model(vocab_size, embed_size, num_layers, heads, dropout, forward_expansion, num_experts)\n",
        "\n",
        "# Dummy input (batch_size=32, seq_len=10)\n",
        "dummy_input = torch.randint(0, vocab_size, (10, 32))\n",
        "output = model(dummy_input)\n",
        "\n",
        "print(\"Transformer2 Model Output Shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xA6WoY2UbClD",
        "outputId": "b815f489-b3e3-4ce5-8e66-2c8d932646a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformer2 Model Output Shape: torch.Size([10, 32, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. **Training Expert Vectors with RL**\n",
        "\n",
        "- **Objective**: Each expert vector is specialized for a specific task or domain (e.g., coding, math, reasoning).\n",
        "  \n",
        "- **Process**:\n",
        "  - A smaller, more parameter-efficient model (or a component of the main model) is fine-tuned using **Reinforcement Learning (RL)**.\n",
        "  - The **Singular Value Fine-Tuning (SVF)** approach adjusts only the **singular values** of the model's weight matrices.\n",
        "  - During training, the model interacts with the environment, receiving rewards based on performance (correct outputs, task success, etc.).\n",
        "  - The RL algorithm (e.g., REINFORCE) optimizes these singular values to create a compact representation specialized for the task.\n",
        "\n",
        "- **Result**: The fine-tuned singular values form a **task-specific expert vector** that captures the specialized behavior required for that task.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **How Expert Vectors are Used**\n",
        "\n",
        "- **Modularity**: Each expert vector is stored independently and can be dynamically combined or selected during inference.\n",
        "\n",
        "- **Dynamic Adaptation**:  \n",
        "  - When a new input arrives, Transformer2 first **classifies** the task (using a dispatch system or prompt engineering).  \n",
        "  - It then **activates the relevant expert vector(s)** to modify the model’s behavior without retraining.\n",
        "\n",
        "- **Mixture of Experts (MoE) Strategy**:  \n",
        "  - In more complex tasks, Transformer2 can combine multiple expert vectors (via weighted mixtures) for richer adaptability.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Why This Is Powerful**\n",
        "\n",
        "- **Efficiency**: Only the singular values are trained, which requires **fewer parameters** than retraining the full model.  \n",
        "- **Scalability**: New expert vectors can be added over time without impacting existing ones.  \n",
        "- **Continual Learning**: The model can adapt to new tasks without **catastrophic forgetting**.  \n",
        "\n",
        "---\n",
        "\n",
        "### 🔑 **Simplified Analogy**  \n",
        "Think of each **expert vector** as a **\"skill module\"**. You don’t need to rebuild a robot every time it learns a new task; instead, you just install a specialized tool. Similarly, Transformer2 installs specialized \"skills\" via these expert vectors."
      ],
      "metadata": {
        "id": "ty3shmlfbzl7"
      }
    }
  ]
}