rStar: Mutual Reasoning for Small Language Models (SLMs)
Key Idea

Uses a self-play approach where a target SLM generates solutions, and a peer SLM verifies them. This mutual verification strengthens reasoning without fine-tuning or superior models.
Implementation

    Generation via Monte Carlo Tree Search (MCTS):
        Represents reasoning as a search tree:
            Root node: Problem statement.
            Edges: Reasoning steps.
            Leaf nodes: Final solutions.
        Defines a rich action space to enhance solution diversity:
            Proposing one-step thoughts.
            Breaking down the problem into sub-questions.
            Rephrasing unclear questions.

    Mutual Verification:
        Discriminator Model:
            A second SLM validates reasoning trajectories generated by the first model.
            Given partial reasoning steps as context, it completes the reasoning and compares its outcome with the original trajectory.
        Trajectories with consistent results are deemed reliable.

    Final Selection:
        From verified trajectories, selects the one with the highest reward (calculated based on reasoning confidence).

Experiments

    Datasets:
        GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA.

    Key Observations:
        Rich action space significantly improved solution generation.
        Mutual verification increased reliability and accuracy of selected solutions.

    Results:
        Boosted reasoning accuracy in smaller models like LLaMA2-7B by up to 51% on GSM8K.
        Achieved performance comparable to fine-tuned models using unsupervised techniques.

Objective: To enhance reasoning capabilities in small language models (SLMs) without fine-tuning or superior models.

Key Concept:

    Implements a self-play mutual reasoning process where one model generates solutions using Monte Carlo Tree Search (MCTS), and another similar model validates these solutions through mutual agreement.
    The mutually agreed solutions are deemed more reliable and correct.

Implementation:

    Generation (MCTS): The target model explores reasoning trajectories through a diverse set of actions inspired by human problem-solving (e.g., decomposing questions, rephrasing).
    Discrimination: A secondary model verifies the generated trajectories by attempting to replicate reasoning steps and comparing outcomes.
    Selection: The final solution is chosen based on mutual consistency and a reward mechanism.

Advantages: Efficiently explores the solution space and provides robust validation, enabling smaller models to perform reasoning tasks effectively.

Performance: Achieved significant improvements in reasoning benchmarks (e.g., GSM8K, MATH, and others), often rivaling larger models.